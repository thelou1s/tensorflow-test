{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2958 - accuracy: 0.9134\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1435 - accuracy: 0.9564\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1069 - accuracy: 0.9672\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0904 - accuracy: 0.9724\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0750 - accuracy: 0.9767\n",
      "313/313 - 0s - loss: 0.0717 - accuracy: 0.9791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07168534398078918, 0.9790999889373779]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 安装 TensorFlow\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 载入并准备好 MNIST 数据集。将样本从整数转换为浮点数：\n",
    "\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# 将模型的各层堆叠起来，以搭建 tf.keras.Sequential 模型。为训练选择优化器和损失函数：\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 训练并验证模型：\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/conda/lib/python3.7/site-packages (2.4.3)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from keras) (1.6.0)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py->keras) (1.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import keras  \n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/62033587/cant-import-keras-from-tensorflow\n",
    "\n",
    "from tensorflow import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-345ce270252b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# x_train 和 y_train 是 Numpy 数组 -- 就像在 Scikit-Learn API 中一样。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "# x_train 和 y_train 是 Numpy 数组 -- 就像在 Scikit-Learn API 中一样。\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_on_batch(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.predict(x_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'weather_data/OttawaGatineau_hourly_201806.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dd87e686baaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Get training, test, validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2018\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mtraining_data_large\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2016\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2018\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2017\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-dd87e686baaf>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(year, month)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weather_data/OttawaGatineau_hourly_{0:04d}{1:02d}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mfiltered_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Month\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Day\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Time\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Temp (C)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Dew Point Temp (C)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Rel Hum (%)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Wind Dir (10s deg)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Wind Spd (km/h)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'weather_data/OttawaGatineau_hourly_201806.csv'"
     ]
    }
   ],
   "source": [
    "# Test the model with a small set of data \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.layers as layers\n",
    "import pandas as pd # used for csv reading\n",
    "import math # used to check for nan values while calculating the \"feels like\" temperature\n",
    "import collections # Used to return x and y from data fetching helper function\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def not_empty(val):\n",
    "  if math.isnan(val):\n",
    "    return False\n",
    "  if val == \"\":\n",
    "    return False\n",
    "  if val == \"NaN\":\n",
    "    return False\n",
    "  return True\n",
    "\n",
    "DataSet = collections.namedtuple('DataSet', ['x', 'y'])\n",
    "\n",
    "def get_data(year,month):\n",
    "  file = open('weather_data/OttawaGatineau_hourly_{0:04d}{1:02d}.csv'.format(year,month),'r')\n",
    "  data = pd.read_csv(file, delimiter=',',quotechar='\"',quoting=1)\n",
    "  filtered_frame = data[[\"Month\",\"Day\",\"Time\",\"Temp (C)\",\"Dew Point Temp (C)\",\"Rel Hum (%)\",\"Wind Dir (10s deg)\",\"Wind Spd (km/h)\"]]\n",
    "  filtered_frame['Time'] = filtered_frame['Time'].map(lambda time: int(time[:-3])) #convert time to hour as int\n",
    "  x = filtered_frame.values\n",
    "\n",
    "  temperature = data[[\"Temp (C)\",\"Hmdx\",\"Wind Chill\"]]\n",
    "  y = temperature.apply(lambda row: row[\"Hmdx\"] if not_empty(row[\"Hmdx\"]) else row[\"Wind Chill\"] if not_empty(row[\"Wind Chill\"]) else row[\"Temp (C)\"], axis=1).values\n",
    "  return DataSet(x,y)\n",
    "\n",
    "\n",
    "def get_all_data(start_year, start_month, end_year, end_month):\n",
    "  year = start_year\n",
    "  month = start_month\n",
    "  monthly_data = get_data(year, month)\n",
    "  x = monthly_data.x\n",
    "  y = monthly_data.y\n",
    "  month += 1\n",
    "  if (month == 13):\n",
    "    month = 1\n",
    "    year += 1\n",
    "  while (year <= end_year and month <= end_month):\n",
    "    monthly_data = get_data(year, month)\n",
    "    x = np.append(x,monthly_data.x, axis=0)\n",
    "    y = np.append(y,monthly_data.y, axis=0)\n",
    "    month += 1\n",
    "    if (month == 13):\n",
    "      month = 1\n",
    "      year += 1\n",
    "  return DataSet(x,y)\n",
    "\n",
    "# Get training, test, validation data\n",
    "training_data = get_data(2018,6)\n",
    "training_data_large = get_all_data(2016,4,2018,6)\n",
    "test_data = get_data(2017,6)\n",
    "validation_data = get_data(2016,6)\n",
    "\n",
    "\n",
    "# Create models\n",
    "models = []\n",
    "\n",
    "# Reduce the learning rate\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.001, momentum=0.0, decay=0.0001, nesterov=False)\n",
    "\n",
    "# Model 1\n",
    "model1 = tf.keras.Sequential()\n",
    "model1.add(layers.BatchNormalization())\n",
    "model1.add(layers.Dense(30, activation='hard_sigmoid', kernel_initializer='random_uniform'))\n",
    "model1.add(layers.Dense(1))\n",
    "model1.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "models.append(model1)\n",
    "\n",
    "for model in models:\n",
    "  print ('********************************** Model evaluation **********************************')\n",
    "  # more epochs\n",
    "  model.fit(training_data.x, training_data.y, epochs=200, batch_size=10, verbose=1,validation_data=(validation_data.x, validation_data.y))\n",
    "  print (len(model.layers))\n",
    "\n",
    "  for layer in model.layers:\n",
    "    print ('weights', layer.get_weights()[0])\n",
    "    print ('biases', layer.get_weights()[1])\n",
    "  predictions = model.predict(test_data.x)\n",
    "\n",
    "  print('Examples:')\n",
    "  for i in range(0,10):\n",
    "    index = i * 10\n",
    "    #print('Example ',i, test_data.x[index])\n",
    "    print('Prediction', index, predictions[index][0], ' actual ', test_data.y[index])\n",
    "\n",
    "  score = model.evaluate(test_data.x,test_data.y, verbose=1) #bah\n",
    "  print('Test loss:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "422/422 [==============================] - 51s 120ms/step - loss: 0.3628 - accuracy: 0.8905 - val_loss: 0.0809 - val_accuracy: 0.9793\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 50s 117ms/step - loss: 0.1119 - accuracy: 0.9661 - val_loss: 0.0569 - val_accuracy: 0.9848\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 51s 120ms/step - loss: 0.0844 - accuracy: 0.9739 - val_loss: 0.0520 - val_accuracy: 0.9840\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 51s 121ms/step - loss: 0.0701 - accuracy: 0.9790 - val_loss: 0.0435 - val_accuracy: 0.9880\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 52s 123ms/step - loss: 0.0622 - accuracy: 0.9809 - val_loss: 0.0391 - val_accuracy: 0.9887\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 50s 119ms/step - loss: 0.0558 - accuracy: 0.9823 - val_loss: 0.0379 - val_accuracy: 0.9903\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 52s 124ms/step - loss: 0.0495 - accuracy: 0.9844 - val_loss: 0.0360 - val_accuracy: 0.9907\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 51s 120ms/step - loss: 0.0477 - accuracy: 0.9849 - val_loss: 0.0326 - val_accuracy: 0.9913\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 50s 119ms/step - loss: 0.0444 - accuracy: 0.9860 - val_loss: 0.0307 - val_accuracy: 0.9918\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 52s 122ms/step - loss: 0.0410 - accuracy: 0.9869 - val_loss: 0.0312 - val_accuracy: 0.9918\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 52s 123ms/step - loss: 0.0400 - accuracy: 0.9868 - val_loss: 0.0328 - val_accuracy: 0.9910\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 50s 118ms/step - loss: 0.0378 - accuracy: 0.9879 - val_loss: 0.0294 - val_accuracy: 0.9923\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 49s 117ms/step - loss: 0.0343 - accuracy: 0.9885 - val_loss: 0.0312 - val_accuracy: 0.9910\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 52s 123ms/step - loss: 0.0353 - accuracy: 0.9886 - val_loss: 0.0297 - val_accuracy: 0.9905\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 51s 120ms/step - loss: 0.0319 - accuracy: 0.9897 - val_loss: 0.0312 - val_accuracy: 0.9918\n",
      "Test loss: 0.027894405648112297\n",
      "Test accuracy: 0.9905999898910522\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/examples/vision/mnist_convnet/ #\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin   entrypoint.sh  init.sh  media  proc  run_jupyter.sh  sys\t\tusr\n",
      "boot  etc\t     lib      mnt    root  sbin\t\t   test_folder\tvar\n",
      "dev   home\t     lib64    opt    run   srv\t\t   tmp\n"
     ]
    }
   ],
   "source": [
    "!ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /test_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.25.0 for Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
